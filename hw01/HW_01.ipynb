{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task01 numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.0000004   7.50000011]\n",
      " [22.6000009  16.95000023]]\n",
      "Output layer:  [[0.92414184 0.07585816]\n",
      " [0.99649482 0.00350518]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x=np.exp(x-np.max(x))\n",
    "    return e_x/e_x.sum()\n",
    "\n",
    "x1=np.array([1.0,2.0,3.0])\n",
    "x2=np.array([4.0,5.0,6.0])\n",
    "\n",
    "x=np.stack([x1,x2])\n",
    "\n",
    "w1=np.array([[0.1,0.2,0.3,0.4],[0.5,0.6,0.7,0.8],[0.9,1.0,1.1,1.2]],dtype=np.float32)\n",
    "w2=np.array([[0.2,0.1],[0.4,0.5],[0.6,0.2],[0.8,0.7]],dtype=np.float32)\n",
    "\n",
    "hidden_layer=np.maximum(np.dot(x,),0)\n",
    "\n",
    "\n",
    "output_Layer=np.exp(np.dot(hidden_layer,w2))/np.sum(np.exp(np.dot(hidden_layer,w2)),axis=1,keepdims=True)\n",
    "\n",
    "print(\"Output layer: \",output_Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task 01 pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available:  True\n",
      "Device:  cuda\n",
      "Output layer:  tensor([[0.9241, 0.0759],\n",
      "        [0.9965, 0.0035]])\n"
     ]
    }
   ],
   "source": [
    "# Input layer with 3 nodes\n",
    "# Hidden layer with 4 nodes and ReLU activation\n",
    "# Output layer with 2 nodes and softmax activation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "gpu_avail=torch.cuda.is_available()\n",
    "print(\"GPU available: \",gpu_avail)\n",
    "if gpu_avail:\n",
    "    device=torch.device(\"cuda\")\n",
    "else:\n",
    "    device=torch.device(\"cpu\")\n",
    "print(\"Device: \",device)\n",
    "\n",
    "# Generate intputData\n",
    "x1=torch.tensor([1.0,2.0,3.0])\n",
    "x2=torch.tensor([4.0,5.0,6.0])\n",
    "\n",
    "x=torch.stack([x1,x2])\n",
    "\n",
    "# Generate weights\n",
    "w1=torch.tensor([[0.1,0.2,0.3,0.4],[0.5,0.6,0.7,0.8],[0.9,1.0,1.1,1.2]],dtype=torch.float32)\n",
    "w2=torch.tensor([[0.2,0.1],[0.4,0.5],[0.6,0.2],[0.8,0.7]],dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Generate hidden layer\n",
    "hidden_layer=F.relu(torch.matmul(x,w1))\n",
    "\n",
    "output_layer=F.softmax(torch.matmul(hidden_layer,w2),dim=1)\n",
    "\n",
    "\n",
    "print(\"Output layer: \",output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task02 numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6379850004327519\n",
      "Gradient of w1:\n",
      " [[-0.00443694  0.00443694 -0.01774778 -0.00443694]\n",
      " [-0.00343802  0.00343802 -0.01375207 -0.00343802]\n",
      " [-0.00243909  0.00243909 -0.00975637 -0.00243909]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Activation & loss helpers ---\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x, axis=1):\n",
    "    x = x - np.max(x, axis=axis, keepdims=True)\n",
    "    e = np.exp(x)\n",
    "    return e / np.sum(e, axis=axis, keepdims=True)\n",
    "\n",
    "def cross_entropy(output, target):\n",
    "    return -np.sum(target * np.log(output), axis=1).mean()\n",
    "\n",
    "# --- Data & weights (no bias) ---\n",
    "x = np.stack([\n",
    "    np.array([0.1, 0.2, 0.3]),\n",
    "    np.array([0.4, 0.5, 0.6])\n",
    "])  # shape (2,3)\n",
    "\n",
    "w1 = np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2]\n",
    "])\n",
    "w2 = np.array([\n",
    "    [0.2, 0.1],\n",
    "    [0.4, 0.5],\n",
    "    [0.6, 0.2],\n",
    "    [0.8, 0.7]\n",
    "])\n",
    "\n",
    "# --- Forward pass ---\n",
    "hidden = relu(x.dot(w1))               # (2,4)\n",
    "output = softmax(hidden.dot(w2), axis=1)  # (2,2)\n",
    "\n",
    "target = np.array([[0,1],[1,0]])       # one-hot labels\n",
    "\n",
    "loss = cross_entropy(output, target)\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "# --- Backprop to compute ∂Loss/∂w1 ---\n",
    "batch_size = x.shape[0]\n",
    "\n",
    "# dL/d_logits_out = (output - target) / batch_size\n",
    "delta_out = (output - target) / batch_size  # (2,2)\n",
    "\n",
    "# Propagate through w2 into hidden\n",
    "delta_hidden = delta_out.dot(w2.T)          # (2,4)\n",
    "delta_hidden[hidden <= 0] = 0               # ReLU gradient\n",
    "\n",
    "# Finally gradient of w1: x^T · delta_hidden\n",
    "grad_w1 = x.T.dot(delta_hidden)             # (3,4)\n",
    "\n",
    "print(\"Gradient of w1:\\n\", grad_w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task02 pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available:  True\n",
      "Device:  cuda\n",
      "Gradient of w1:  tensor([[-0.0044,  0.0044, -0.0177, -0.0044],\n",
      "        [-0.0034,  0.0034, -0.0138, -0.0034],\n",
      "        [-0.0024,  0.0024, -0.0098, -0.0024]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "gpu_avail=torch.cuda.is_available()\n",
    "print(\"GPU available: \",gpu_avail)\n",
    "if gpu_avail:\n",
    "    device=torch.device(\"cuda\")\n",
    "else:\n",
    "    device=torch.device(\"cpu\")\n",
    "print(\"Device: \",device)\n",
    "\n",
    "def cross_entropy_loss(output,target):\n",
    "    return -(target*torch.log(output)).sum(dim=1).mean()\n",
    "\n",
    "# Generate data\n",
    "x1=torch.tensor([0.1,0.2,0.3],dtype=torch.float32)\n",
    "x2=torch.tensor([0.4,0.5,0.6],dtype=torch.float32)\n",
    "x=torch.stack([x1,x2])\n",
    "\n",
    "w1=torch.tensor([[0.1,0.2,0.3,0.4],[0.5,0.6,0.7,0.8],[0.9,1.0,1.1,1.2]],dtype=torch.float32,requires_grad=True)\n",
    "w2=torch.tensor([[0.2,0.1],[0.4,0.5],[0.6,0.2],[0.8,0.7]],dtype=torch.float32)\n",
    "\n",
    "hidden_layer=F.relu(torch.matmul(x,w1))\n",
    "\n",
    "output_layer=F.softmax(torch.matmul(hidden_layer,w2),dim=1)\n",
    "\n",
    "# print(\"Output layer: \",output_layer)\n",
    "\n",
    "target=torch.tensor([[0,1],[1,0]],dtype=torch.float32)\n",
    "loss=cross_entropy_loss(output_layer,target)\n",
    "\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"Gradient of w1: \",w1.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task03 numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task03 pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
